{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import jax.config as config\n",
    "config.update('jax_enable_x64', True)\n",
    "import numpy as np\n",
    "\n",
    "import pytensor\n",
    "from pytensor import tensor as pt\n",
    "from pytensor.graph.basic import Apply\n",
    "from pytensor.graph import Apply, Op\n",
    "\n",
    "import jax\n",
    "from jax import grad, jit, vmap, value_and_grad, random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import prepare_inference\n",
    "\n",
    "import pyhf\n",
    "pyhf.set_backend('jax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([60., 60., 60., 60., 60.], dtype=float64), [0.0, 1.0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "obs = jnp.array([70.]*N)\n",
    "model = pyhf.simplemodels.correlated_background([10]*N,[50]*N, [45]*N, [55]*N)\n",
    "model.expected_actualdata(model.config.suggested_init()),model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VJPOp(pt.Op):\n",
    "    itypes = [pt.dvector,pt.dvector]  \n",
    "    otypes = [pt.dvector]\n",
    "\n",
    "    def __init__(self, vjp_func):\n",
    "        self.vjp_func = vjp_func\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (at, vector) = inputs\n",
    "        results = self.vjp_func(at, vector)\n",
    "\n",
    "        if len(outputs) == 1:\n",
    "            outputs[0][0] = np.asarray(results, dtype = np.float64)\n",
    "\n",
    "        for i, r in enumerate(results):\n",
    "            outputs[i][0] = np.asarray(r, dtype = np.float64)\n",
    "\n",
    "\n",
    "class ExpDataOp(pt.Op):\n",
    "    itypes = [pt.dvector]  \n",
    "    otypes = [pt.dvector]\n",
    "\n",
    "    def __init__(self, fwd_func):\n",
    "        self.fwd_func = fwd_func\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (parameters, ) = inputs\n",
    "        results = self.fwd_func(parameters)\n",
    "\n",
    "        if len(outputs) == 1:\n",
    "                outputs[0][0] = np.asarray(results, dtype = np.float64)\n",
    "                return\n",
    "        for i, r in enumerate(results):\n",
    "                outputs[i][0] = np.asarray(r, dtype = np.float64)\n",
    "\n",
    "    def grad(self, at_vector, vector):\n",
    "        return [vjp_op(at_vector[0],vector[0])]\n",
    "    \n",
    "def _pyhf_forward(x):\n",
    "    return model.expected_actualdata(x)\n",
    "\n",
    "pyhf_fwd_func = jax.jit(_pyhf_forward)\n",
    "pyhf_vjp_func = jax.jit(lambda at, vector: jax.vjp(_pyhf_forward, at)[1](vector))\n",
    "                \n",
    "fwd_op = ExpDataOp(pyhf_fwd_func)\n",
    "vjp_op = VJPOp(pyhf_vjp_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_inference\n",
    "from prepare_inference import get_target\n",
    "def priors2pymc(prepared_model):\n",
    "    unconstr_pars, norm_pars, poiss_pars = [], [], []\n",
    "    norm_mu, norm_sigma = [], []\n",
    "    poiss_alpha, poiss_beta = [], []\n",
    "    model = prepared_model['model']\n",
    "    obs = prepared_model['obs']\n",
    "    prior_dict = prepared_model['priors']\n",
    "    precision = prepared_model['precision']\n",
    "        \n",
    "    for key in prior_dict.keys():\n",
    "        sub_dict = prior_dict[key]\n",
    "\n",
    "    ## Unconstrained\n",
    "        if sub_dict['type'] == 'unconstrained':\n",
    "            unconstr_pars.extend(pm.Gamma('Unconstrained', alpha=sub_dict['input'][0], beta=sub_dict['input'][1]))\n",
    "        pass\n",
    "\n",
    "    ## Normal and Poisson constraints            \n",
    "        if sub_dict['type'] == 'normal':\n",
    "            norm_mu.append(sub_dict['input'][0])\n",
    "            norm_sigma.append(sub_dict['input'][1])\n",
    "\n",
    "        if sub_dict['type'] == 'poisson':\n",
    "            poiss_alpha.append(sub_dict['input'][0])\n",
    "            poiss_beta.append(sub_dict['input'][1])\n",
    "\n",
    "    if np.array(norm_mu, dtype=object).size != 0:\n",
    "        norm_pars.extend(pm.Normal('Normals', mu=list(np.concatenate(norm_mu)), sigma=list(np.concatenate(norm_sigma))))\n",
    "\n",
    "    if np.array(poiss_alpha, dtype=object).size != 0:\n",
    "        poiss_pars.extend(pm.Gamma('Gammas', alpha=list(np.concatenate(poiss_alpha)), beta=list(np.concatenate(poiss_beta))))\n",
    "\n",
    "    pars = []\n",
    "    for i in [unconstr_pars, norm_pars, poiss_pars]:\n",
    "        i = np.array(i)\n",
    "        if i.size != 0:\n",
    "            pars.append(i)\n",
    "    pars = np.concatenate(pars)\n",
    "    target = get_target(model)\n",
    "    final = pt.as_tensor_variable(pars[target.argsort()].tolist())\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_inference\n",
    "unconstr_dict = {\n",
    "    'mu': {'type': 'unconstrained', 'input': [[1.], [1.]]}\n",
    "}\n",
    "prior_dict = prepare_inference.prepare_priors(model, unconstr_dict)\n",
    "prepared_model = prepare_inference.prepare_model(model=model, observations=obs, precision=1, priors=prior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 150 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 100 tune and 150 draw iterations (100 + 150 draws total) took 0 seconds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "different number of dimensions on data and dims: 3 vs 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m data \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mPoisson(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, expected, observed\u001b[39m=\u001b[39mobs)\n\u001b[1;32m      7\u001b[0m post_data \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39msample(\u001b[39m150\u001b[39m,  chains \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, tune\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m post_pred \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39;49msample_posterior_predictive(post_data)\n\u001b[1;32m      9\u001b[0m prior_pred \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39msample_prior_predictive(\u001b[39m150\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/pymc/sampling/forward.py:673\u001b[0m, in \u001b[0;36msample_posterior_predictive\u001b[0;34m(trace, model, var_names, sample_dims, random_seed, progressbar, return_inferencedata, extend_inferencedata, predictions, idata_kwargs, compile_kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m         ikwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39minplace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    672\u001b[0m     \u001b[39mreturn\u001b[39;00m pm\u001b[39m.\u001b[39mpredictions_to_inference_data(ppc_trace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mikwargs)\n\u001b[0;32m--> 673\u001b[0m idata_pp \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39;49mto_inference_data(posterior_predictive\u001b[39m=\u001b[39;49mppc_trace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mikwargs)\n\u001b[1;32m    675\u001b[0m \u001b[39mif\u001b[39;00m extend_inferencedata \u001b[39mand\u001b[39;00m idata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     idata\u001b[39m.\u001b[39mextend(idata_pp)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/pymc/backends/arviz.py:485\u001b[0m, in \u001b[0;36mto_inference_data\u001b[0;34m(trace, prior, posterior_predictive, log_likelihood, coords, dims, sample_dims, model, save_warmup, include_transformed)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(trace, InferenceData):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m trace\n\u001b[0;32m--> 485\u001b[0m \u001b[39mreturn\u001b[39;00m InferenceDataConverter(\n\u001b[1;32m    486\u001b[0m     trace\u001b[39m=\u001b[39;49mtrace,\n\u001b[1;32m    487\u001b[0m     prior\u001b[39m=\u001b[39;49mprior,\n\u001b[1;32m    488\u001b[0m     posterior_predictive\u001b[39m=\u001b[39;49mposterior_predictive,\n\u001b[1;32m    489\u001b[0m     log_likelihood\u001b[39m=\u001b[39;49mlog_likelihood,\n\u001b[1;32m    490\u001b[0m     coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    491\u001b[0m     dims\u001b[39m=\u001b[39;49mdims,\n\u001b[1;32m    492\u001b[0m     sample_dims\u001b[39m=\u001b[39;49msample_dims,\n\u001b[1;32m    493\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    494\u001b[0m     save_warmup\u001b[39m=\u001b[39;49msave_warmup,\n\u001b[1;32m    495\u001b[0m     include_transformed\u001b[39m=\u001b[39;49minclude_transformed,\n\u001b[1;32m    496\u001b[0m )\u001b[39m.\u001b[39;49mto_inference_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/pymc/backends/arviz.py:406\u001b[0m, in \u001b[0;36mInferenceDataConverter.to_inference_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_inference_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    397\u001b[0m     \u001b[39m\"\"\"Convert all available data to an InferenceData object.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39m    Note that if groups can not be created (e.g., there is no `trace`, so\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m    the `posterior` and `sample_stats` can not be extracted), then the InferenceData\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m    will not have those groups.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     id_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    404\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mposterior\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior_to_xarray(),\n\u001b[1;32m    405\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msample_stats\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_stats_to_xarray(),\n\u001b[0;32m--> 406\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mposterior_predictive\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposterior_predictive_to_xarray(),\n\u001b[1;32m    407\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictions_to_xarray(),\n\u001b[1;32m    408\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpriors_to_xarray(),\n\u001b[1;32m    409\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobserved_data\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserved_data_to_xarray(),\n\u001b[1;32m    410\u001b[0m     }\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictions:\n\u001b[1;32m    412\u001b[0m         id_dict[\u001b[39m\"\u001b[39m\u001b[39mpredictions_constant_data\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstant_data_to_xarray()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/arviz/data/base.py:65\u001b[0m, in \u001b[0;36mrequires.__call__.<locals>.wrapped\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m((\u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, prop_i) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m prop_i \u001b[39min\u001b[39;00m prop)):\n\u001b[1;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/pymc/backends/arviz.py:327\u001b[0m, in \u001b[0;36mInferenceDataConverter.posterior_predictive_to_xarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior_predictive\n\u001b[1;32m    326\u001b[0m dims \u001b[39m=\u001b[39m {var_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_dims \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims\u001b[39m.\u001b[39mget(var_name, []) \u001b[39mfor\u001b[39;00m var_name \u001b[39min\u001b[39;00m data}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mreturn\u001b[39;00m dict_to_dataset(\n\u001b[1;32m    328\u001b[0m     data, library\u001b[39m=\u001b[39;49mpymc, coords\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoords, dims\u001b[39m=\u001b[39;49mdims, default_dims\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_dims\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/arviz/data/base.py:307\u001b[0m, in \u001b[0;36mdict_to_dataset\u001b[0;34m(data, attrs, library, coords, dims, default_dims, index_origin, skip_event_dims)\u001b[0m\n\u001b[1;32m    305\u001b[0m data_vars \u001b[39m=\u001b[39m {}\n\u001b[1;32m    306\u001b[0m \u001b[39mfor\u001b[39;00m key, values \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 307\u001b[0m     data_vars[key] \u001b[39m=\u001b[39m numpy_to_data_array(\n\u001b[1;32m    308\u001b[0m         values,\n\u001b[1;32m    309\u001b[0m         var_name\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    310\u001b[0m         coords\u001b[39m=\u001b[39;49mcoords,\n\u001b[1;32m    311\u001b[0m         dims\u001b[39m=\u001b[39;49mdims\u001b[39m.\u001b[39;49mget(key),\n\u001b[1;32m    312\u001b[0m         default_dims\u001b[39m=\u001b[39;49mdefault_dims,\n\u001b[1;32m    313\u001b[0m         index_origin\u001b[39m=\u001b[39;49mindex_origin,\n\u001b[1;32m    314\u001b[0m         skip_event_dims\u001b[39m=\u001b[39;49mskip_event_dims,\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[39mreturn\u001b[39;00m xr\u001b[39m.\u001b[39mDataset(data_vars\u001b[39m=\u001b[39mdata_vars, attrs\u001b[39m=\u001b[39mmake_attrs(attrs\u001b[39m=\u001b[39mattrs, library\u001b[39m=\u001b[39mlibrary))\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/arviz/data/base.py:254\u001b[0m, in \u001b[0;36mnumpy_to_data_array\u001b[0;34m(ary, var_name, coords, dims, default_dims, index_origin, skip_event_dims)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m# filter coords based on the dims\u001b[39;00m\n\u001b[1;32m    253\u001b[0m coords \u001b[39m=\u001b[39m {key: xr\u001b[39m.\u001b[39mIndexVariable((key,), data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray(coords[key])) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dims}\n\u001b[0;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m xr\u001b[39m.\u001b[39;49mDataArray(ary, coords\u001b[39m=\u001b[39;49mcoords, dims\u001b[39m=\u001b[39;49mdims)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/xarray/core/dataarray.py:428\u001b[0m, in \u001b[0;36mDataArray.__init__\u001b[0;34m(self, data, coords, dims, name, attrs, indexes, fastpath)\u001b[0m\n\u001b[1;32m    426\u001b[0m data \u001b[39m=\u001b[39m _check_data_shape(data, coords, dims)\n\u001b[1;32m    427\u001b[0m data \u001b[39m=\u001b[39m as_compatible_data(data)\n\u001b[0;32m--> 428\u001b[0m coords, dims \u001b[39m=\u001b[39m _infer_coords_and_dims(data\u001b[39m.\u001b[39;49mshape, coords, dims)\n\u001b[1;32m    429\u001b[0m variable \u001b[39m=\u001b[39m Variable(dims, data, attrs, fastpath\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    430\u001b[0m indexes, coords \u001b[39m=\u001b[39m _create_indexes_from_coords(coords)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/xarray/core/dataarray.py:142\u001b[0m, in \u001b[0;36m_infer_coords_and_dims\u001b[0;34m(shape, coords, dims)\u001b[0m\n\u001b[1;32m    140\u001b[0m     dims \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(dims)\n\u001b[1;32m    141\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(dims) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(shape):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdifferent number of dimensions on data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand dims: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dims)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dims:\n",
      "\u001b[0;31mValueError\u001b[0m: different number of dimensions on data and dims: 3 vs 2"
     ]
    }
   ],
   "source": [
    "with pm.Model():\n",
    "    pars = priors2pymc(prepared_model)    \n",
    "    params = pm.Deterministic('params',pars)\n",
    "    expected = pm.Deterministic('expected',fwd_op(params))\n",
    "    data = pm.Poisson('data', expected, observed=obs)\n",
    "\n",
    "    post_data = pm.sample(150,  chains = 1, tune=100)\n",
    "    post_pred = pm.sample_posterior_predictive(post_data)\n",
    "    prior_pred = pm.sample_prior_predictive(150)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhf_pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
