{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyhf\n",
    "pyhf.set_backend('jax')\n",
    "\n",
    "import pymc as pm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pytensor\n",
    "from pytensor import tensor as pt\n",
    "from pytensor.graph.basic import Apply\n",
    "from pytensor.graph import Apply, Op\n",
    "\n",
    "# import aesara\n",
    "import aesara.tensor as at\n",
    "# from aesara.graph.op import Op\n",
    "from aesara.link.jax.dispatch import jax_funcify\n",
    "\n",
    "import jax\n",
    "from jax import grad, jit, vmap, value_and_grad, random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(1, '/Users/malinhorstmann/Documents/pyhf_pymc/src')\n",
    "import MH_inference\n",
    "import HMC_inference\n",
    "import prepare_inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SRee_SRmm_Srem.json') as serialized:\n",
    "    spec = json.load(serialized)\n",
    "\n",
    "workspace = pyhf.Workspace(spec)\n",
    "model = workspace.model()\n",
    "obs = workspace.data(model, include_auxdata=False)\n",
    "nBins = len(model.expected_actualdata(model.config.suggested_init()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def processed_expData(parameters):\n",
    "    # a = jnp.stack([jax.jit(model.expected_actualdata(p))[i] for i in range(nBins)])\n",
    "    a = jnp.stack([model.expected_actualdata(parameters)[i] for i in range(nBins)])\n",
    "    return a\n",
    "\n",
    "@jax.jit\n",
    "def vjp_expData(parameters, vector):\n",
    "    _,back = jax.vjp(processed_expData, parameters)\n",
    "    return back(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, (18,))\n"
     ]
    }
   ],
   "source": [
    "pars = prepare_inference.priors2pymc(prepared_model)\n",
    "print(pars.type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_op(func, itypes, otypes):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    @jax.jit\n",
    "    def vjp_func(parameters, vector):\n",
    "        _,back = jax.vjp(func, parameters)\n",
    "        return back(vector)\n",
    "\n",
    "    class JaxVJPOp(Op):\n",
    "        __props__ = (\"jax_vjp_func\",)\n",
    "\n",
    "        def __init__(self):\n",
    "            self.jax_vjp_func = vjp_func\n",
    "            self.itypes = itypes + otypes\n",
    "            self.otypes = itypes\n",
    "            super().__init__()\n",
    "\n",
    "        def perform(self, node, inputs, outputs):\n",
    "\n",
    "            results = self.jax_vjp_func(*(jnp.asarray(x) for x in inputs))\n",
    "\n",
    "            if not isinstance(results, (list, tuple)):\n",
    "                results = (results,)\n",
    "\n",
    "            for i, r in enumerate(results):\n",
    "                outputs[i][0] = np.asarray(r)\n",
    "\n",
    "\n",
    "    jax_grad_op = JaxVJPOp()\n",
    "                \n",
    "    @jax_funcify.register(JaxVJPOp)\n",
    "    def jax_funcify_JaxGradOp(op):\n",
    "        return op.jax_vjp_func\n",
    "\n",
    "    @jax.jit\n",
    "    def fwd_func(fwd_inputs):\n",
    "        return func(fwd_inputs)\n",
    "    \n",
    "    class JaxOp(Op):\n",
    "        __props__ = (\"fwd_func\",)\n",
    "\n",
    "        def __init__(self):\n",
    "            self.fwd_func = fwd_func\n",
    "            self.itypes = itypes\n",
    "            self.otypes = otypes\n",
    "            super().__init__()\n",
    "\n",
    "        def perform(self, node, inputs, outputs):\n",
    "            results = self.fwd_func(*(jnp.asarray(x) for x in inputs))\n",
    "            if len(outputs) == 1:\n",
    "                outputs[0][0] = np.asarray(results)\n",
    "                return\n",
    "            for i, r in enumerate(results):\n",
    "                outputs[i][0] = np.asarray(r)\n",
    "\n",
    "        def grad(self, inputs, vectors):\n",
    "            return [jax_grad_op(inputs[0], vectors[0])]\n",
    "\n",
    "    # @jax_funcify.register(JaxOp)\n",
    "    # def jax_funcify_JaxOp(op):\n",
    "    #     return op.fwd_func\n",
    "\n",
    "    jax_op = JaxOp()\n",
    "    \n",
    "    return jax_op, jax_grad_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, (18,))\n",
      "[ 5.22021957e-02  7.65032179e-03  9.50228104e-02  2.61874674e-05\n",
      " -3.44132928e-05 -4.50589683e-06  5.89919525e-06  2.11944936e-05\n",
      " -7.58614712e-06  1.74513037e-02  1.90408675e-01 -2.65716482e-05\n",
      " -3.39078458e-06  4.72531697e-06  3.81544561e-06  1.61576618e-05\n",
      "  4.09487406e-06 -9.99838836e-07]\n",
      "[0.47245307 0.01439832 0.2079121 ]\n"
     ]
    }
   ],
   "source": [
    "### Appling the Op to model.expected_actualdata\n",
    "op, grad_op = HMC_inference.make_op(\n",
    "    processed_expData,\n",
    "    (at.TensorType(dtype=np.float64, shape=(len(model.config.par_map),)),),\n",
    "    (at.TensorType(dtype=np.float64, shape=(nBins,)),),\n",
    ")\n",
    "\n",
    "a = np.linspace(0.01, 1, len(model.config.par_names)).tolist()\n",
    "pars = at.as_tensor_variable(a)\n",
    "print(pars.type)\n",
    "print(grad_op(pars, at.constant([1.0, 1.0, 1.0])).eval())\n",
    "\n",
    "print(op(pars).eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the priors for sampling\n",
    "    # Unconstrained parameters\n",
    "unconstr_dict = {\n",
    "    'uncon1': {'type': 'unconstrained', 'type2': 'normal', 'input': [[1], [0.1]]}\n",
    "    }\n",
    "\n",
    "    # Create dictionary with all priors (unconstrained, constrained by normal and poisson)\n",
    "prior_dict = prepare_inference.prepare_priors(model, unconstr_dict)\n",
    "\n",
    "    # dictionary with keys 'model', 'obs', 'priors', 'precision'\n",
    "prepared_model = prepare_inference.prepare_model(model=model, observations=obs, precision=0.10, priors=prior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, (18,))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid input types for Op JaxOp{fwd_func=<CompiledFunction of <function make_op.<locals>.fwd_func at 0x28bcee0d0>>}:\nInput 1/1: Expected TensorType(float64, (18,)), got TensorType(float64, (18,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pars \u001b[39m=\u001b[39m prepare_inference\u001b[39m.\u001b[39mpriors2pymc(prepared_model)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(pars\u001b[39m.\u001b[39mtype)\n\u001b[0;32m----> 7\u001b[0m mu \u001b[39m=\u001b[39m op(pars)\n\u001b[1;32m      8\u001b[0m main \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mNormal(\u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m, mu\u001b[39m=\u001b[39mmu)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/aesara/graph/op.py:297\u001b[0m, in \u001b[0;36mOp.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Construct an `Apply` node using :meth:`Op.make_node` and return its outputs.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[1;32m    257\u001b[0m \u001b[39mThis method is just a wrapper around :meth:`Op.make_node`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m return_list \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mreturn_list\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 297\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_node(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mcompute_test_value \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    300\u001b[0m     compute_test_value(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/aesara/graph/op.py:241\u001b[0m, in \u001b[0;36mOp.make_node\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitypes)\u001b[39m}\u001b[39;00m\u001b[39m inputs but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    238\u001b[0m     expected_type\u001b[39m.\u001b[39mis_super(var\u001b[39m.\u001b[39mtype)\n\u001b[1;32m    239\u001b[0m     \u001b[39mfor\u001b[39;00m var, expected_type \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitypes)\n\u001b[1;32m    240\u001b[0m ):\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid input types for Op \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    244\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(inputs)\u001b[39m}\u001b[39;00m\u001b[39m: Expected \u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m             \u001b[39mfor\u001b[39;00m i, (inp, out) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[1;32m    246\u001b[0m                 \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitypes, (inp\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs)),\n\u001b[1;32m    247\u001b[0m                 start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    248\u001b[0m             )\n\u001b[1;32m    249\u001b[0m             \u001b[39mif\u001b[39;00m inp \u001b[39m!=\u001b[39m out\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m Apply(\u001b[39mself\u001b[39m, inputs, [o() \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39motypes])\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid input types for Op JaxOp{fwd_func=<CompiledFunction of <function make_op.<locals>.fwd_func at 0x28bcee0d0>>}:\nInput 1/1: Expected TensorType(float64, (18,)), got TensorType(float64, (18,))"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "with pm.Model():\n",
    "    \n",
    "    pars = prepare_inference.priors2pymc(prepared_model)\n",
    "    print(pars.type)\n",
    "\n",
    "    mu = op(pars)\n",
    "    main = pm.Normal(\"main\", mu=mu)#, observed=obs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VJPCustomOp(Op):\n",
    "\n",
    "    def make_node(self, vjp_func, parameters, vector):\n",
    "        self.func = vjp_expData\n",
    "        inputs = [pt.as_tensor_variable(x), pt.as_tensor_variable(vector)]\n",
    "        outputs = [inputs[0].type()]\n",
    "        \n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (parameters, vector) = inputs\n",
    "        results = vjp_expData(parameters, vector)\n",
    "\n",
    "        if not isinstance(results, (list, tuple)):\n",
    "                results = (results,)\n",
    "                \n",
    "        for i, r in enumerate(results):\n",
    "            outputs[i][0] = np.asarray(r)\n",
    "\n",
    "vjp_custom_op = VJPCustomOp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, (18,))\n",
      "TensorType(float64, (18,))\n",
      "[-0.45348924  0.40669046  0.09532455]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [Unconstrained, Normals, test]\n",
      "NUTS: [Unconstrained, Normals, test]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 100 draw iterations (4_000 + 400 draws total) took 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Testing if at-type tensors work\n",
    "a = np.linspace(0.01, 1, len(model.config.par_names)).tolist()\n",
    "pars = at.as_tensor_variable(a)\n",
    "print(pars.type)\n",
    "\n",
    "# Testing if pm-type Tensors work\n",
    "pars = prepare_inference.priors2pymc(prepared_model)\n",
    "print(pars.type)\n",
    "\n",
    "print(vjp_custom_op(vjp_func=vjp_expData, parameters=pars, vector=pt.as_tensor_variable([1.0, 1.0, 1.0])).eval())\n",
    "\n",
    "# Sampling the gradient\n",
    "with pm.Model():\n",
    "    pars = prepare_inference.priors2pymc(prepared_model)\n",
    "    mu = vjp_custom_op(vjp_func=vjp_expData, parameters=pars, vector=pt.as_tensor_variable([1.0, 1.0, 1.0])).eval()\n",
    "    pm.Normal(\"test\", mu=mu, sigma=0.1)\n",
    "    pm.sample(100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The non-gradient Op (with grad node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOp(Op):\n",
    "    \n",
    "    def make_node(self, func, parameters):\n",
    "        self.func = processed_expData\n",
    "        inputs = [pt.as_tensor_variable(parameters)]\n",
    "        outputs = [inputs[0].type()]\n",
    "\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        results = vjp_expData(parameters, vector)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_op(func, itypes, otypes):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    @jax.jit\n",
    "    def vjp_func(fwd_inputs, vector):\n",
    "        _,back = jax.vjp(func,fwd_inputs)\n",
    "        return back(vector)\n",
    "\n",
    "    class JaxVJPOp(Op):\n",
    "        __props__ = (\"jax_vjp_func\",)\n",
    "\n",
    "        def __init__(self):\n",
    "            self.jax_vjp_func = vjp_func\n",
    "            self.itypes = itypes + otypes\n",
    "            self.otypes = itypes\n",
    "            super().__init__()\n",
    "\n",
    "        def perform(self, node, inputs, outputs):\n",
    "\n",
    "            results = self.jax_vjp_func(*(jnp.asarray(x) for x in inputs))\n",
    "\n",
    "            if not isinstance(results, (list, tuple)):\n",
    "                results = (results,)\n",
    "\n",
    "            for i, r in enumerate(results):\n",
    "                outputs[i][0] = np.asarray(r)\n",
    "\n",
    "\n",
    "    jax_grad_op = JaxVJPOp()\n",
    "                \n",
    "    @jax_funcify.register(JaxVJPOp)\n",
    "    def jax_funcify_JaxGradOp(op):\n",
    "        return op.jax_vjp_func\n",
    "\n",
    "    @jax.jit\n",
    "    def fwd_func(fwd_inputs):\n",
    "        return func(fwd_inputs)\n",
    "    \n",
    "    class JaxOp(Op):\n",
    "        __props__ = (\"fwd_func\",)\n",
    "\n",
    "        def __init__(self):\n",
    "            self.fwd_func = fwd_func\n",
    "            self.itypes = itypes\n",
    "            self.otypes = otypes\n",
    "            super().__init__()\n",
    "\n",
    "        def perform(self, node, inputs, outputs):\n",
    "            results = self.fwd_func(*(jnp.asarray(x) for x in inputs))\n",
    "            if len(outputs) == 1:\n",
    "                outputs[0][0] = np.asarray(results)\n",
    "                return\n",
    "            for i, r in enumerate(results):\n",
    "                outputs[i][0] = np.asarray(r)\n",
    "\n",
    "        def grad(self, inputs, vectors):\n",
    "            return [jax_grad_op(inputs[0], vectors[0])]\n",
    "\n",
    "    @jax_funcify.register(JaxOp)\n",
    "    def jax_funcify_JaxOp(op):\n",
    "        return op.fwd_func\n",
    "\n",
    "    jax_op = JaxOp()\n",
    "    \n",
    "    return jax_op, jax_grad_op"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_op_jax(x):\n",
    "    return jnp.exp(x)\n",
    "\n",
    "jitted_custom_op_jax = jax.jit(custom_op_jax)\n",
    "\n",
    "def vjp_custom_op_jax(x, gz):\n",
    "    _, vjp_fn = jax.vjp(custom_op_jax, x)\n",
    "    return vjp_fn(gz)[0]\n",
    "\n",
    "jitted_vjp_custom_op_jax = jax.jit(vjp_custom_op_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOp(Op):\n",
    "    def make_node(self, ):\n",
    "        # Create a PyTensor node specifying the number and type of inputs and outputs\n",
    "\n",
    "        # We convert the input into a PyTensor tensor variable\n",
    "        inputs = [pt.as_tensor_variable(x)]\n",
    "        # Output has the same type and shape as `x`\n",
    "        outputs = [inputs[0].type()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # Evaluate the Op result for a specific numerical input\n",
    "\n",
    "        # The inputs are always wrapped in a list\n",
    "        (x,) = inputs\n",
    "        result = jitted_custom_op_jax(x)\n",
    "        # The results should be assigned inplace to the nested list\n",
    "        # of outputs provided by PyTensor. If you have multiple\n",
    "        # outputs and results, you should assign each at outputs[i][0]\n",
    "        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n",
    "\n",
    "    def grad(self, inputs, output_gradients):\n",
    "        # Create a PyTensor expression of the gradient\n",
    "        (x,) = inputs\n",
    "        (gz,) = output_gradients\n",
    "        # We reference the VJP Op created below, which encapsulates\n",
    "        # the gradient operation\n",
    "        return [vjp_custom_op(x, gz)]\n",
    "\n",
    "\n",
    "class VJPCustomOp(Op):\n",
    "    def make_node(self, x, gz):\n",
    "        # Make sure the two inputs are tensor variables\n",
    "        inputs = [pt.as_tensor_variable(x), pt.as_tensor_variable(gz)]\n",
    "        # Output has the shape type and shape as the first input\n",
    "        outputs = [inputs[0].type()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (x, gz) = inputs\n",
    "        result = jitted_vjp_custom_op_jax(x, gz)\n",
    "        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n",
    "\n",
    "# Instantiate the Ops\n",
    "custom_op = CustomOp()\n",
    "vjp_custom_op = VJPCustomOp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.gradient.verify_grad(custom_op, (np.arange(5, dtype=\"float64\"),), rng=np.random.default_rng())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/Users/malinhorstmann/anaconda3/envs/pyhf_pymc/lib/python3.9/site-packages/multipledispatch/dispatcher.py:27: AmbiguityWarning: \n",
      "Ambiguities exist in dispatched function _unify\n",
      "\n",
      "The following signatures may result in ambiguous behavior:\n",
      "\t[ConstrainedVar, object, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\t[ConstrainedVar, object, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\t[ConstrainedVar, Var, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\t[ConstrainedVar, Var, Mapping], [object, ConstrainedVar, Mapping]\n",
      "\n",
      "\n",
      "Consider making the following additions:\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "\n",
      "@dispatch(ConstrainedVar, ConstrainedVar, Mapping)\n",
      "def _unify(...)\n",
      "  warn(warning_text(dispatcher.name, ambiguities), AmbiguityWarning)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [x]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal(\"x\", shape=(3,))\n",
    "    y = pm.Deterministic(\"y\", custom_op(x))  # HERE IS WHERE WE USE THE CUSTOM OP!\n",
    "    z = pm.Normal(\"z\", y, observed=[1, 2, 0])\n",
    "    pm.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhf_pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
